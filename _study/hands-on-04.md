---
title: "Hands-On Machine Learning chap 04"
permalink: /study/
last_modified_at: 2019-08-07T16:42:52-04:00
toc: true
---

## 4. 모델 훈련

- 머신러닝 작동 원리를 이해해보자.
  - 머신러닝 모델과 훈련 알고리즘이 어떻게 작동하는지 잘 이해하고 있으면, 효율적으로 분석하는 데 도움이 된다.
    - 적절한 모델
    - 올바른 훈련 알고리즘
    - 작업에 맞는 좋은 하이퍼파라미터
<br><br>

- 모델을 훈련시키는 두 가지 방법을 살펴본다.
  - 직접 계산할 수 있는 공식을 사용하는 방법
  - 경사 하강법(GD)이라 불리는 반복적인 최적화 방식을 사용하는 방법
<br><br>

## 4.1 선형 회귀

- 선형 회귀 모델의 예측
  - $$ \hat{y}=\theta_{0}+\theta_{1}x_{1}+\cdots+\theta_{n}x_{n} $$
  - $$ \hat{y} $$ : 예측값
  - $$ n $$ : 특성 수
  - $$ \theta_{0} $$ : 편향
<br><br>

- 선형 회귀 모델의 예측 (벡터 형태)
  - $$ \hat{y}=h_\theta(\mathbf{x})=\theta^T\cdot\mathbf{x} $$
  - $$ h_\theta $$ : 가설함수
  - $$ \theta=[\theta_{0}\ \theta_{1}\ \cdots\ \theta_{n}]^T $$ : 모델의 파라미터 벡터 ($$ \theta_{0} $$ 포함)
  - $$ \mathbf{x}=[x_{0}\ x_{1}\ \cdots\ x_{n}]^T $$ : 특성 벡터 ($$ x_{0} $$는 항상 1)
<br><br>

- 모델을 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다.
<br><br>

- 먼저 모델이 훈련 데이터에 얼마나 잘 들어 맞는지 측정해야 한다.
  - 비용 함수를 최소화하는 $$ \theta $$를 찾아야 한다.
<br><br>

- 선형 회귀 모델의 MSE 비용 함수
  - $$ MSE(\mathbf{X}, h_\theta)=\cfrac{1}{m}\sum_{i=1}^{m} (\theta^T\cdot\mathbf{x}^\left(i\right) - y^\left(i\right))^2 $$
<br><br>

- 비용 함수를 최소화하는 모델 파라미터의 조합을 파라미터 공간이라고 한다.
<br><br>

**4.1.1 정규방정식**

- 비용함수를 최소화하는 $$ \theta $$ 값을 찾기 위해 해석적인 방법이 있다.
<br><br>

- 정규방정식
  - $$ \hat{\theta}=(\mathbf{X}^T\cdot\mathbf{X})^{-1}\cdot\mathbf{X}^T\cdot\mathbf{y} $$
  - $$ \hat{\theta} $$ : 비용 함수를 최소화하는 $$ \theta $$ 값
  - $$ \mathbf{y} $$ : 타깃벡터
<br><br>

**4.1.2 계산 복잡도**

- 정규방정식은 역행렬을 계산한다.
  - 일반적으로 역행렬 계산 복잡도는 $$ O(n^{2.4}) $$ 에서 $$ O(n^3) $$ 사이이다.
  - 따라서, 특성 수가 두 배로 늘어나면 계산 시간이 대략 8배로 증가할 수 있다.
<br><br>

- 정규방정식의 복잡도는 훈련 세트의 샘플 수에는 선형적으로 증가한다.($$ O(m) $$)
  - 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리할 수 있다.
<br><br>

- 정규방정식으로 학습된 선형 회귀 모델은 예측이 매우 빠르다.
  - 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다.
<br><br>

## 4.2 경사 하강법

- 경사 하강법(GD:Gradient Descent)은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.
  - 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것이다.
    - 파라미터 벡터 $$ \theta $$를 임의의 값으로 시작한다(무작위 초기화).
    - $$ \theta $$에 대해 비용 함수의 현재 그래디언트를 계산한다.
    - 그래디언트가 감소하는 방향(비용 함수가 감소하는 방향)으로 진행한다.
    - 그래디언트가 0이 되면 최솟값에 도달한 것이다(수렴).
<br><br>

- 경사 하강법에서는 학습률(learning rate)이 중요한 하이퍼파라미터다. <br>
  ![4-3](images/4-3.png)
  - 학습률은 스텝의 크기이다.
  - 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 한다.
  - 학습률이 너무 크면 알고리즘이 더 큰 값으로 발산할지도 모른다.
<br><br>

- 경사 하강법에는 두 가지 문제점이 있다. <br>
  ![4-6](images/4-6.png)
  - 무작위 초기화 때문에 알고리즘이 왼쪽에서 시작하면 전역 최솟값보다 덜 좋은 지역 최솟값에 수렴한다.
  - 알고리즘이 오른쪽에서 시작하 평탄한 지역을 지나기 위해 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못한다.
  - 다행히 선형 회귀를 위한 MSE 비용 함수는 볼록 함수이다.
    - 지역 최솟값이 없고, 하나의 전역 최솟값만 있다.
    - 연속 함수이고 기울기가 갑자기 변하지 않는다.
    - 따라서, 경사 하강법이 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다(학습률이 너무 높지 않고 충분한 시간이 주어지면).
<br><br>

- 특성 스케일이 매우 다르면 길쭉한 모양일 수 있다. <br>
  ![4-7](images/4-7.png)
  - 왼쪽 그래프에서는 최솟값으로 곧장 진행하여 빠르게 도달했다.
  - 오른쪽 그래프에서는 평면한 골짜기를 길게 돌아서가 최솟값에 도달하는데 시간이 오래 걸린다.
  - 따라서, 경사 하강법을 사용할 때는 반드시 모든 특성이 같은 스케일을 갖도록 해야 한다.
<br><br>

**4.2.1 배치 경사 하강법**

- 경사 하강법을 구현하려면 각 모델 파라미터 $$ \theta_{j} $$에 대해 비용 함수의 그래디언트를 계산해야 한다.
  - $$ \theta_{j} $$가 변경될 때 비용 함수가 얼마나 바뀌는지 계산한다(편도함수).
<br><br>

- 비용 함수의 편도함수
  - $$ \cfrac{\partial}{\partial \theta_j}MSE(\theta)=
    \cfrac{2}{m} \sum_{i=1}^{m} (\theta^T \cdot \mathbf{x}^\laft(i\right) - y^\left(i\right)) x_j^\left(i\right) $$
<br><br>

- 비용 함수의 그래디언트 벡터
  - $$ \nabla_{\theta}MSE(\theta)=
    \begin{bmatrix}
      \cfrac{\partial}{\partial \theta_0}MSE(\theta) \\
      \cfrac{\partial}{\partial \theta_1}MSE(\theta) \\
      \vdots \\
      \cfrac{\partial}{\partial \theta_n}MSE(\theta)
    \end{bmatrix}=
    \cfrac{2}{m}\mathbf{X}^T\cdot(\mathbf{X}\cdot\theta-\mathbf{y}) $$
<br><br>

- 배치 경사 하강법은 매 경사 하강법 스텝에서 전체 훈련 세트 $$ \mathbf{X} $$에 대해 계산한다.
  - 즉, 매 스텝에서 훈련 데이터 전체를 사용한다.
  - 따라서, 큰 훈련 세트에서는 아주 느리다.
  - 그러나, 특성 수에 민감하지 않다.
    - 수십만 개의 특성에서 선형 회귀를 훈련시키려면 정규방정식보다 경사 하강법을 사용하는 편이 훨씬 빠르다.
<br><br>

- 경사 하강법의 스텝
  - 위로 향하는 그래디언트 벡터가 구해지면 반대 방향인 아래로 가야 한다.
  - $$ \theta^\left({next\ step}\right)=\theta-\eta\nabla_{\theta}MSE(\theta) $$
  - $$ \eta $$ : 내려가는 스텝의 크기(learning rate)
<br><br>

- 적절한 학습률을 찾으려면 그리드 탐색을 사용해야 하지만, 수렴하는 데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 한다.
  - 반복 횟수가 너무 작으면 최적점에 도달하기 전에 알고리즘이 멈춘다.
  - 반복 횟수가 너무 크면 모델 파라미터가 더 이상 변하지 않는 동안 시간을 낭비한다.
  - 해결책으로, 반복 횟수를 아주 크게 지정하고 그래디언트 벡터가 아주 작아지면 알고리즘을 중지한다.
<br><br>

**4.2.2 확률적 경사 하강법**

- 확률적 경사 하강법(SGD)은 매 스텝에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산한다.
  - 매 반복에서 매우 적은 데이터만 처리하기 때문에 알고리즘이 훨씬 빠르다.
  - 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련시킬 수 있다.
<br><br>

- 확률적(무작위)이기 때문에 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정하다.
  - 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치면서 평균적으로 감소한다.
  - 알고리즘이 멈출 때 좋은 파라미터가 구해지지만 최적치는 아니다.
<br><br>

- 비용 함수가 매우 불규칙할 경우 알고리즘이 지역 최솟값을 건너뛸 수 있도록 도와준다.
  - 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높다.
<br><br>

- 무작위성의 딜레마를 해결하기 위해 학습률을 점진적으로 감소시키는 방법이 있다.
  - 시작할 때는 학습률을 크게 하고, 점차 작게 줄여서 알고리즘이 전역 최솟값에 도달하게 한다.
  - 매 반복에서 학습률을 결정하는 함수를 학습 스케줄라고 한다.
  - 학습률이 너무 빠르게 줄면 지역 최솟값에 갇히거나 최솟값까지 가는 중간에 멈출 수 있다.
  - 학습률이 너무 천천히 줄면 오랫동안 최솟값 주변을 맴돌거나 훈련을 너무 일찍 중지해서 지역 최솟값에 머물 수 있다.
<br><br>

- 한 반복에서 훈련세트의 샘플 수만큼 되풀이 될 때, 각 반복을 에포크라고 한다.
<br><br>

**4.2.3 미니배치 경사 하강법**

- 미니배치 경사 하강법은 임의의 작은 샘플 세트에 대해 그래디언트를 계산한다.
  - 미니배치를 어느 정도 크게 하면 파라미터 공간에서 SGD보다 덜 불규칙하게 움직인다.
  - 따라서, SGD보다 최솟값에 더 가까이 도달하게 된다.
<br><br>

**정리**

- 배치 경사 하강법에는 매 스텝에서 많은 시간이 소요된다.
- 확률적 경사 하강법과 미니배치 경사 하강법은 적절한 학습 스케줄을 사용하면 최솟값에 도달한다.

| 알고리즘 | 훈련 샘플 수가 클 때 | 외부 메모리 학습 지원 | 특성 수가 클 때 | 하이퍼파라미터 수 | 스케일 조정 필요 |
|:-|:-|:-|:-|:-|:-|
| 정규방정식 | 빠름 | No | 느림 | 0 | No |
| 배치 경사 하강법 | 느림 | No | 빠름 | 2 | Yes |
| 확률적 경사 하강법 | 빠름 | Yes | 빠름 | 2 이상 | Yes |
| 미니배치 경사 하강법 | 빠름 | Yes | 빠름 | 2 이상 | Yes |

<br><br>

## 4.3 다항 회귀

- 비선형 데이터를 학습하는 데 선형 모델을 사용할 수 있다.
  - 각 특성의 거듭제곱을 새로운 특성으로 추가하여 선형 모델을 훈련시킨다.
  - 다항 회귀에서는 특성이 여러 개일 때 특성 사이의 관계를 찾을 수 있다(특성 간의 교차항을 추가하기 때문).
  - `PolynomialFeatures` : 주어진 차수까지 각 특성의 거듭제곱과 특성 간의 모든 교차항 만드는 함수 (sklearn)
<br><br>

## 4.4 학습 곡선

- 모델의 일반화 성능을 추정하기 위해 교차 검증을 사용했다.
  - 훈련 데이터에서 성능이 좋지만 교차 검증 점수가 나쁘면 모델이 과대적합된 것이다.
  - 양쪽 모두 좋지 않으면 과소적합된 것이다.

- 모델의 일반화 성능을 추정하기 위해 학습 곡선을 보는 방법도 있다.
  -
